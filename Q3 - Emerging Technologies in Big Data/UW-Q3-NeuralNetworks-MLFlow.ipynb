{"cells":[{"cell_type":"markdown","source":["### Pete Champlin\n### Big Data 230 A\n#### Week 7 Assignment\n6/6/2020"],"metadata":{}},{"cell_type":"code","source":["import keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\n\n\n# Use TensorFlow Backend\nimport tensorflow as tf\ntf.set_random_seed(42) # For reproducibility\n\n# Print out Keras version\nprint(keras.__version__)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2.2.5\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Configure MLflow Experiment\n#mlflow_experiment_id = 2102416\n\n# Including MLflow\nimport mlflow\nimport mlflow.keras\nimport os\nprint(\"MLflow Version: %s\" % mlflow.__version__)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MLflow Version: 1.7.0\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["import warnings\nwarnings.filterwarnings(\"ignore\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["#### Prepare training and test data"],"metadata":{}},{"cell_type":"code","source":["# -----------------------------------------------------------\n\nnum_classes = 10\n\n# -----------------------------------------------------------\n# Image Datasets\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">x_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["#### Define parameterized CNN function"],"metadata":{}},{"cell_type":"code","source":["def runCNN(activation, choose_optimizer, lr, epochs, batch_size, verbose = 0):\n  \n  # Documentaion: https://keras.io/guides/sequential_model/\n  \n#The input layer is a grey scale image of 28x28 pixels. \n#The first convolution layer maps one grayscale image to 32 feature maps using the activation function\n#The second convolution layer maps the image to 64 feature maps using the activation function\n#The pooling layer down samples image by 2x so you have a 14x14 matrix \n#The first dropout layer delete random neurons (regularization technique to avoid overfitting)\n#The fully connected feed-forward maps the features with 128 neurons in the hidden layer\n#The second dropout layer delete random neurons (regularization technique to avoid overfitting)\n#Apply softmax with 10 hidden layers to identify digit.\n\n  # Building up our CNN\n  model = Sequential() # type: keras.engine.sequential.Sequential\n  \n  # Convolution Layer\n  model.add(Conv2D( # 2D convolution layer\n              32, # filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution)\n              kernel_size=(3, 3), # An tuple/list of 2 integers, specifying the height and width of the 2D convolution window\n              activation=activation,\n              input_shape=input_shape) # Only needed when first layer added\n           ) \n  \n  # Convolution layer\n  model.add(Conv2D(\n              64, \n              kernel_size=(3, 3), \n              activation=activation))\n  \n  # MaxPooling2D layer\n  # Pooling with stride (2, 2)\n  #  Downsamples the input representation by taking the maximum value over the window defined by pool_size for each dimension along the\n  #  features axis. The window is shifted by strides in each dimension. The resulting output when using \"valid\" padding option has a\n  #  shape(number of rows or columns) of: output_shape = (input_shape - pool_size + 1) / strides)\n  model.add(MaxPooling2D(\n              pool_size=(2, 2)))\n  \n  # Delete neuron randomly while training (remain 75%)\n  #   Regularization technique to avoid overfitting\n  model.add(Dropout(0.25))\n  \n  # Flatten layer \n  model.add(Flatten())\n  \n  # Fully connected Layer\n  model.add(Dense(128, activation=activation))\n  \n  # Delete neuron randomly while training (remain 50%) \n  #   Regularization technique to avoid overfitting\n  model.add(Dropout(0.5))\n  \n  # Apply Softmax\n  model.add(Dense(num_classes, activation='softmax'))\n\n  # change optimizer parameters\n  if choose_optimizer == 'adadelta':\n      optimizer = keras.optimizers.Adadelta(lr=lr, rho=0.95, epsilon=None, decay=0.0)\n  elif choose_optimizer == 'sgd':\n      optimizer = keras.optimizers.SGD(lr=lr, momentum=0.0, decay=0.0, nesterov=False)\n  elif choose_optimizer == 'nag':\n      optimizer = keras.optimizers.SGD(lr=lr, momentum=0.0, decay=0.0, nesterov=True)\n  elif choose_optimizer == 'rmsprop':\n      optimizer = keras.optimizers.RMSprop(lr=lr, rho=0.95, epsilon=None, decay=0.0)\n  elif choose_optimizer == 'adam':\n      optimizer = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n      \n  # Log MLflow\n  #with mlflow.start_run(experiment_id = mlflow_experiment_id) as run:\n  with mlflow.start_run() as run:\n  \n    # Loss function (crossentropy) and Optimizer\n    model.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=optimizer,\n              metrics=['accuracy'])\n\n    # Fit our model\n    model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=verbose,\n          validation_data=(x_test, y_test))\n\n    #model.summary()\n    \n    # Evaluate our model\n    score = model.evaluate(x_test, y_test, verbose=0)\n\n    # Log Parameters\n    mlflow.log_param(\"activation function\", activation)\n    mlflow.log_param(\"optimizer\", choose_optimizer)\n    mlflow.log_param(\"learning rate\", lr)\n    mlflow.log_metric(\"test loss\", score[0])\n    mlflow.log_metric(\"test accuracy\", score[1])\n    \n    # Log Model\n    mlflow.keras.log_model(model, \"model\")\n    \n  # Return\n  return score"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["#### Run models with various optimizers and learning rates"],"metadata":{}},{"cell_type":"code","source":["# REMINDER: input_shape = (28, 28, 1), training sample size = 60000, and test sample size = 10000\n\n#import warnings\nwarnings.filterwarnings(\"ignore\")\n\nactivation = 'relu' # 'sigmoid', 'tanh'\nbatch_size = 128 # Batch size - the number of training samples to work through before the modelâ€™s internal parameters are updated\nepochs = 1 # Epoch - the number of complete passes through the training dataset\n\nchoose_optimizers = ['adadelta', 'sgd', 'nag', 'rmsprop', 'adam']\nlearning_rates = [10.0, 1.0, 0.1, 0.01, 0.001]\n\nfor choose_optimizer in choose_optimizers:\n  for learning_rate in learning_rates:\n    cnn_score = runCNN(activation, choose_optimizer, learning_rate, epochs, batch_size)\n    print('choose_optimizer:', choose_optimizer)\n    print('learning_rate:', learning_rate)\n    print('Test loss:', cnn_score[0])\n    print('Test accuracy:', cnn_score[1])\n    print('')\n     "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">choose_optimizer: adadelta\nlearning_rate: 10.0\nTest loss: 14.461155044555664\nTest accuracy: 0.1028\n\nchoose_optimizer: adadelta\nlearning_rate: 1.0\nTest loss: 0.06157040466587059\nTest accuracy: 0.9812\n\nchoose_optimizer: adadelta\nlearning_rate: 0.1\nTest loss: 0.21058260089159012\nTest accuracy: 0.9381\n\nchoose_optimizer: adadelta\nlearning_rate: 0.01\nTest loss: 0.737422237110138\nTest accuracy: 0.8394\n\nchoose_optimizer: adadelta\nlearning_rate: 0.001\nTest loss: 2.2441894371032713\nTest accuracy: 0.4159\n\nchoose_optimizer: sgd\nlearning_rate: 10.0\nTest loss: 14.573981651306152\nTest accuracy: 0.0958\n\nchoose_optimizer: sgd\nlearning_rate: 1.0\nTest loss: 14.461155044555664\nTest accuracy: 0.1028\n\nchoose_optimizer: sgd\nlearning_rate: 0.1\nTest loss: 14.454707752990723\nTest accuracy: 0.1032\n\nchoose_optimizer: sgd\nlearning_rate: 0.01\nTest loss: 0.31326896134614945\nTest accuracy: 0.9088\n\nchoose_optimizer: sgd\nlearning_rate: 0.001\nTest loss: 2.148486209106445\nTest accuracy: 0.5667\n\nchoose_optimizer: nag\nlearning_rate: 10.0\nTest loss: 14.28869146270752\nTest accuracy: 0.1135\n\nchoose_optimizer: nag\nlearning_rate: 1.0\nTest loss: 14.573981651306152\nTest accuracy: 0.0958\n\nchoose_optimizer: nag\nlearning_rate: 0.1\nTest loss: 14.491779391479492\nTest accuracy: 0.1009\n\nchoose_optimizer: nag\nlearning_rate: 0.01\nTest loss: 0.289064549356699\nTest accuracy: 0.9161\n\nchoose_optimizer: nag\nlearning_rate: 0.001\nTest loss: 2.068292176437378\nTest accuracy: 0.6583\n\nchoose_optimizer: rmsprop\nlearning_rate: 10.0\nTest loss: 14.538521841430665\nTest accuracy: 0.098\n\nchoose_optimizer: rmsprop\nlearning_rate: 1.0\nTest loss: 14.491779391479492\nTest accuracy: 0.1009\n\nchoose_optimizer: rmsprop\nlearning_rate: 0.1\nTest loss: 14.490167601013184\nTest accuracy: 0.101\n\nchoose_optimizer: rmsprop\nlearning_rate: 0.01\nTest loss: 14.535298265075683\nTest accuracy: 0.0982\n\nchoose_optimizer: rmsprop\nlearning_rate: 0.001\nTest loss: 0.12257258466528728\nTest accuracy: 0.9662\n\nchoose_optimizer: adam\nlearning_rate: 10.0\nTest loss: 14.490167601013184\nTest accuracy: 0.101\n\nchoose_optimizer: adam\nlearning_rate: 1.0\nTest loss: 14.241948974609375\nTest accuracy: 0.1164\n\nchoose_optimizer: adam\nlearning_rate: 0.1\nTest loss: 14.461155044555664\nTest accuracy: 0.1028\n\nchoose_optimizer: adam\nlearning_rate: 0.01\nTest loss: 0.06260221842193278\nTest accuracy: 0.9813\n\nchoose_optimizer: adam\nlearning_rate: 0.001\nTest loss: 0.05278770954753272\nTest accuracy: 0.9829\n\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["With a small batch and epoch, the adam and adadelta optimizers achieved the highest accuracies, over 98%.\n\nI'll run the three highest-performing models with higher batch sizes."],"metadata":{}},{"cell_type":"code","source":["warnings.filterwarnings(\"ignore\")\n\nactivation = 'relu'\nbatch_size = 1000\nepochs = 3\nchoose_optimizer = 'adam'\nlearning_rate = 0.001\ncnn_score = runCNN(activation, choose_optimizer, learning_rate, epochs, batch_size)\n\nactivation = 'relu'\nbatch_size = 1000\nepochs = 3\nchoose_optimizer = 'adam'\nlearning_rate = 0.01\ncnn_score = runCNN(activation, choose_optimizer, learning_rate, epochs, batch_size)\n\nactivation = 'relu'\nbatch_size = 1000\nepochs = 3\nchoose_optimizer = 'adadelta'\nlearning_rate = 1.0\ncnn_score = runCNN(activation, choose_optimizer, learning_rate, epochs, batch_size)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["With a batch size of 1000 and 3 epochs, adam using a learning rate of 0.01 achieved 98.6% accuracy.\n\nI will run that again with a much higher batch size."],"metadata":{}},{"cell_type":"code","source":["activation = 'relu'\nbatch_size = 20000\nepochs = 10\nchoose_optimizer = 'adam'\nlearning_rate = 0.01\ncnn_score = runCNN(activation, choose_optimizer, learning_rate, epochs, batch_size)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Accuracy: 0.986\n\nLoss: 0.046"],"metadata":{}},{"cell_type":"code","source":["#Example model run with model.summary displayed\n\nactivation = 'relu'\nbatch_size = 1000\nepochs = 1\nchoose_optimizer = 'adam'\nlearning_rate = 0.01\ncnn_score = runCNN(activation, choose_optimizer, learning_rate, epochs, batch_size)\nprint('Test loss:', cnn_score[0])\nprint('Test accuracy:', cnn_score[1])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Model: &#34;sequential_67&#34;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_131 (Conv2D)          (None, 26, 26, 32)        320       \n_________________________________________________________________\nconv2d_132 (Conv2D)          (None, 24, 24, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_66 (MaxPooling (None, 12, 12, 64)        0         \n_________________________________________________________________\ndropout_131 (Dropout)        (None, 12, 12, 64)        0         \n_________________________________________________________________\nflatten_66 (Flatten)         (None, 9216)              0         \n_________________________________________________________________\ndense_131 (Dense)            (None, 128)               1179776   \n_________________________________________________________________\ndropout_132 (Dropout)        (None, 128)               0         \n_________________________________________________________________\ndense_132 (Dense)            (None, 10)                1290      \n=================================================================\nTotal params: 1,199,882\nTrainable params: 1,199,882\nNon-trainable params: 0\n_________________________________________________________________\nTest loss: 0.04877653738911031\nTest accuracy: 0.9842\n</div>"]}}],"execution_count":16}],"metadata":{"name":"Introduction to Neural Networks, MLflow, and SHAP","notebookId":379598621109767},"nbformat":4,"nbformat_minor":0}
