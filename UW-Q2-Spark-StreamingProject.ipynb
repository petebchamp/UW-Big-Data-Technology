{"cells":[{"cell_type":"code","source":["###   REAL-TIME TARGETED ADVERTISING ON MEETUP   ###\n\n####################################\n#REFERENCE DOCUMENTATION\n####################################\n#https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\n#https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html\n#https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\n#https://docs.databricks.com/spark/latest/dataframes-datasets/complex-nested-data.html\n#https://docs.azuredatabricks.net/_static/notebooks/transform-complex-data-types-python.html\n#https://databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html\n#https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n####################################\n\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import from_json, to_json, explode, expr, col, lower, struct #array_contains\n\n#Set up file paths\nrootPath = \"abfss://pbchamp@uwbigdatatechnologies.dfs.core.windows.net/FinalProject\"\narchivePath = rootPath + \"/RSVPArchive\"\narchiveCheckpointPath = rootPath + \"/RSVPArchiveCheckpoint\"\nadvertiserPath = rootPath + \"/Advertiser\"\nadvertiserOutputPath = rootPath + \"/AdvertiserOutput\"\nadvertiserOutputCheckpointPath = rootPath + \"/AdvertiserOutputCheckpoint\"\nadvertiserOutputCheckpoint2Path = rootPath + \"/AdvertiserOutputCheckpoint2\"\n\n#Empty directories (non-prod only)\ndbutils.fs.rm(archivePath, True)\ndbutils.fs.rm(archiveCheckpointPath, True)\ndbutils.fs.rm(advertiserOutputPath, True)\ndbutils.fs.rm(advertiserOutputCheckpointPath, True)\ndbutils.fs.rm(advertiserOutputCheckpoint2Path, True)\n\n#Load static advertiser data\nadvertiserDF = spark.read.json(advertiserPath + \"/advertiser.json\", multiLine = True)\n\n####################################\n#Read Meetup RSVP stream from Kafka\n####################################\nserver = \"ubuntuserver010.westus2.cloudapp.azure.com:9092\"\ninput_topic = \"meetupallrsvps\"\noutput_topic = \"meetupad\"\noffset = \"latest\"\n\nkafkaMeetupDF = (\n  spark\n    .readStream\n    .format(\"kafka\")\n    .option(\"kafka.bootstrap.servers\", server)\n    .option(\"subscribe\", input_topic)\n    .option(\"startingOffsets\", offset)\n    .load()\n)\n\n#Select key and value from Kafka data\nmeetupRawDF = kafkaMeetupDF.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n####################################\n\n#Create nested schema for RSVP JSON\njsonSchemaRSVP = (\n  StructType()\n  .add(\"venue\", StructType()\n       .add(\"lon\", DoubleType())\n       .add(\"lat\", DoubleType()))\n  .add(\"response\", StringType())\n  .add(\"member\", StructType()\n       .add(\"member_id\", LongType()))\n  .add(\"rsvp_id\", LongType())\n  .add(\"mtime\", LongType())\n  .add(\"event\", StructType()\n       .add(\"event_name\", StringType()))\n  .add(\"group\", StructType()\n       .add(\"group_topics\", ArrayType(StructType()\n            .add(\"topic_name\", StringType()))))  \n)\n\n#Structure JSON Meetup data; explode array of topic names/keywords into a row for each; filter for 'yes' responses\nmeetupDF = (\n  meetupRawDF\n     .select(from_json(\"value\", jsonSchemaRSVP)\n     .alias(\"meetup\"))\n      #select individual fields instead of \"*\" to flatten nested JSON\n     .select(\"meetup.rsvp_id\", \"meetup.member.member_id\", \"meetup.mtime\", \"meetup.response\", \n             \"meetup.event.event_name\", \"meetup.venue.lon\", \"meetup.venue.lat\", \n             \"meetup.group.group_topics.topic_name\")\n).select(\"rsvp_id\", \"member_id\", \"mtime\", \"response\", \"event_name\", \"lon\", \"lat\", \n         explode(\"topic_name\").alias(\"keyword\")\n  ).filter(\"response = 'yes'\")\n\n#Join Meetup DF with Advertiser DF on an expression (rather than column(s)), to determine \n# whether the Meetup event is within an advertiser's target lat/lon\njoinPredicate = \"mu.lon between ad.minlon and ad.maxlon AND mu.lat between ad.minlat and ad.maxlat\"\n\nmeetupAdDF = (\n  meetupDF.alias(\"mu\").join(\n    advertiserDF.alias(\"ad\"),\n    expr(joinPredicate)\n  )\n)\n\n#Explode advertiser's keywords and look for match between those and the Meetup group's keywords (topics) \n#Note: Joining the two dataframe's on an expression using the function array_intersect() (e.g. \"size(array_intersect(t.topic_name, f.keywords)) > 0\") \n#  was explored, but the Meetup keywords are mixed case and no efficient way to use lower() function was found.\n# Thus, explode() is used.\nmeetupAd2DF = (\n  meetupAdDF\n    .withColumn(\"adKeywords\", explode(\"keywords\"))\n    .filter(lower(col(\"keyword\")) == col(\"adKeywords\"))\n    .select(\"rsvp_id\", \"member_id\", \"mtime\", \"response\", \"event_name\", \"lon\", \"lat\", \n            \"advertiser\", \"city\", \"keywords\", \"minlon\", \"maxlon\", \"minlat\", \"maxlat\")\n    .distinct()\n)\n\n#Archive Meetup records that meet Advertiser's targets to disk\nmeetupArchiveQuery = (\n  meetupAd2DF\n    .writeStream\n    .format(\"parquet\")\n    .option(\"path\", archivePath)  \n    .option(\"checkpointLocation\", archiveCheckpointPath)\n    .start()\n)\n\nmeetupAdFinalDF = meetupAd2DF.select(\"rsvp_id\", \"member_id\", \"event_name\", \"advertiser\", \"city\").distinct()\n\n#Write key data for advertising to disk (will also send back into Kafka topic for advertising consumption)\nmeetupAdArchiveQuery = (\n  meetupAdFinalDF\n    .writeStream\n    .format(\"parquet\")\n    .option(\"path\", advertiserOutputPath)  \n    .option(\"checkpointLocation\", advertiserOutputCheckpointPath)\n    .start()\n)\n\nmeetupAdKafkaDF = (\n  meetupAdFinalDF\n    .select(\n        col(\"rsvp_id\").cast(\"string\").alias(\"key\"),\n        to_json(struct(\"member_id\", \"event_name\", \"advertiser\", \"city\")).alias(\"value\")\n    )\n)\n\nmeetupAdKafkaQuery = (\n  meetupAdKafkaDF\n    .writeStream\n    .format(\"kafka\")\n    .option(\"kafka.bootstrap.servers\", server)\n    .option(\"topic\", output_topic)\n    .option(\"checkpointLocation\", advertiserOutputCheckpoint2Path)\n    #.outputMode(\"complete\")\n    .start()\n)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["display(meetupAd2DF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>rsvp_id</th><th>member_id</th><th>mtime</th><th>response</th><th>event_name</th><th>lon</th><th>lat</th><th>advertiser</th><th>city</th><th>keywords</th><th>minlon</th><th>maxlon</th><th>minlat</th><th>maxlat</th></tr></thead><tbody><tr><td>1833241628</td><td>212641076</td><td>1584292353986</td><td>yes</td><td>KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch</td><td>-122.399445</td><td>37.788803</td><td>Databricks</td><td>San Francisco</td><td>List(spark, hadoop, big data, machine learning, ai, data pipeline)</td><td>-122.938614</td><td>-121.763077</td><td>37.178392</td><td>38.52294</td></tr><tr><td>1833242400</td><td>187579401</td><td>1584293059792</td><td>yes</td><td>KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch</td><td>-122.399445</td><td>37.788803</td><td>Databricks</td><td>San Francisco</td><td>List(spark, hadoop, big data, machine learning, ai, data pipeline)</td><td>-122.938614</td><td>-121.763077</td><td>37.178392</td><td>38.52294</td></tr><tr><td>1833242400</td><td>187579401</td><td>1584293060000</td><td>yes</td><td>KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch</td><td>-122.399445</td><td>37.788803</td><td>Databricks</td><td>San Francisco</td><td>List(spark, hadoop, big data, machine learning, ai, data pipeline)</td><td>-122.938614</td><td>-121.763077</td><td>37.178392</td><td>38.52294</td></tr><tr><td>1833243534</td><td>193660532</td><td>1584294086745</td><td>yes</td><td>Deep Learning (Tensor Flow, DJL and DL4J ) for Java Developers</td><td>-121.894905</td><td>37.332855</td><td>Databricks</td><td>San Francisco</td><td>List(spark, hadoop, big data, machine learning, ai, data pipeline)</td><td>-122.938614</td><td>-121.763077</td><td>37.178392</td><td>38.52294</td></tr><tr><td>1833245145</td><td>184104333</td><td>1584294910544</td><td>yes</td><td>Deep Learning (Tensor Flow, DJL and DL4J ) for Java Developers</td><td>-121.894905</td><td>37.332855</td><td>Databricks</td><td>San Francisco</td><td>List(spark, hadoop, big data, machine learning, ai, data pipeline)</td><td>-122.938614</td><td>-121.763077</td><td>37.178392</td><td>38.52294</td></tr><tr><td>1833245725</td><td>214186432</td><td>1584295467396</td><td>yes</td><td>Scalable Systems Study Group</td><td>-121.95524</td><td>37.354107</td><td>Databricks</td><td>San Francisco</td><td>List(spark, hadoop, big data, machine learning, ai, data pipeline)</td><td>-122.938614</td><td>-121.763077</td><td>37.178392</td><td>38.52294</td></tr><tr><td>1833245908</td><td>161070382</td><td>1584295631151</td><td>yes</td><td>Deep Learning 101: Artificial Intelligence Based On The Brain</td><td>-122.0096</td><td>37.383553</td><td>Databricks</td><td>San Francisco</td><td>List(spark, hadoop, big data, machine learning, ai, data pipeline)</td><td>-122.938614</td><td>-121.763077</td><td>37.178392</td><td>38.52294</td></tr><tr><td>1833246282</td><td>11411429</td><td>1584295953103</td><td>yes</td><td>KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch</td><td>-122.399445</td><td>37.788803</td><td>Databricks</td><td>San Francisco</td><td>List(spark, hadoop, big data, machine learning, ai, data pipeline)</td><td>-122.938614</td><td>-121.763077</td><td>37.178392</td><td>38.52294</td></tr><tr><td>1833246282</td><td>11411429</td><td>1584295957414</td><td>yes</td><td>KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch</td><td>-122.399445</td><td>37.788803</td><td>Databricks</td><td>San Francisco</td><td>List(spark, hadoop, big data, machine learning, ai, data pipeline)</td><td>-122.938614</td><td>-121.763077</td><td>37.178392</td><td>38.52294</td></tr><tr><td>1833247406</td><td>110703232</td><td>1584296963212</td><td>yes</td><td>Butterflies, big data and the multifarious stressors of the Anthropocene</td><td>-122.27363</td><td>38.023796</td><td>Databricks</td><td>San Francisco</td><td>List(spark, hadoop, big data, machine learning, ai, data pipeline)</td><td>-122.938614</td><td>-121.763077</td><td>37.178392</td><td>38.52294</td></tr><tr><td>1833247571</td><td>185424938</td><td>1584297120000</td><td>yes</td><td>KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch</td><td>-122.399445</td><td>37.788803</td><td>Databricks</td><td>San Francisco</td><td>List(spark, hadoop, big data, machine learning, ai, data pipeline)</td><td>-122.938614</td><td>-121.763077</td><td>37.178392</td><td>38.52294</td></tr><tr><td>1833247571</td><td>185424938</td><td>1584297119855</td><td>yes</td><td>KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch</td><td>-122.399445</td><td>37.788803</td><td>Databricks</td><td>San Francisco</td><td>List(spark, hadoop, big data, machine learning, ai, data pipeline)</td><td>-122.938614</td><td>-121.763077</td><td>37.178392</td><td>38.52294</td></tr></tbody></table></div>"]}}],"execution_count":2},{"cell_type":"code","source":["display(meetupAdFinalDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>rsvp_id</th><th>member_id</th><th>event_name</th><th>advertiser</th><th>city</th></tr></thead><tbody><tr><td>1833241628</td><td>212641076</td><td>KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch</td><td>Databricks</td><td>San Francisco</td></tr><tr><td>1833242400</td><td>187579401</td><td>KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch</td><td>Databricks</td><td>San Francisco</td></tr><tr><td>1833243534</td><td>193660532</td><td>Deep Learning (Tensor Flow, DJL and DL4J ) for Java Developers</td><td>Databricks</td><td>San Francisco</td></tr><tr><td>1833245145</td><td>184104333</td><td>Deep Learning (Tensor Flow, DJL and DL4J ) for Java Developers</td><td>Databricks</td><td>San Francisco</td></tr><tr><td>1833245725</td><td>214186432</td><td>Scalable Systems Study Group</td><td>Databricks</td><td>San Francisco</td></tr><tr><td>1833245908</td><td>161070382</td><td>Deep Learning 101: Artificial Intelligence Based On The Brain</td><td>Databricks</td><td>San Francisco</td></tr><tr><td>1833246282</td><td>11411429</td><td>KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch</td><td>Databricks</td><td>San Francisco</td></tr><tr><td>1833247406</td><td>110703232</td><td>Butterflies, big data and the multifarious stressors of the Anthropocene</td><td>Databricks</td><td>San Francisco</td></tr><tr><td>1833247571</td><td>185424938</td><td>KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch</td><td>Databricks</td><td>San Francisco</td></tr></tbody></table></div>"]}}],"execution_count":3},{"cell_type":"code","source":["display(meetupAdKafkaDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>1833241628</td><td>{\"member_id\":212641076,\"event_name\":\"KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch\",\"advertiser\":\"Databricks\",\"city\":\"San Francisco\"}</td></tr><tr><td>1833242400</td><td>{\"member_id\":187579401,\"event_name\":\"KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch\",\"advertiser\":\"Databricks\",\"city\":\"San Francisco\"}</td></tr><tr><td>1833243534</td><td>{\"member_id\":193660532,\"event_name\":\"Deep Learning (Tensor Flow, DJL and DL4J ) for Java Developers\",\"advertiser\":\"Databricks\",\"city\":\"San Francisco\"}</td></tr><tr><td>1833245145</td><td>{\"member_id\":184104333,\"event_name\":\"Deep Learning (Tensor Flow, DJL and DL4J ) for Java Developers\",\"advertiser\":\"Databricks\",\"city\":\"San Francisco\"}</td></tr><tr><td>1833245725</td><td>{\"member_id\":214186432,\"event_name\":\"Scalable Systems Study Group\",\"advertiser\":\"Databricks\",\"city\":\"San Francisco\"}</td></tr><tr><td>1833245908</td><td>{\"member_id\":161070382,\"event_name\":\"Deep Learning 101: Artificial Intelligence Based On The Brain\",\"advertiser\":\"Databricks\",\"city\":\"San Francisco\"}</td></tr><tr><td>1833246282</td><td>{\"member_id\":11411429,\"event_name\":\"KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch\",\"advertiser\":\"Databricks\",\"city\":\"San Francisco\"}</td></tr><tr><td>1833247406</td><td>{\"member_id\":110703232,\"event_name\":\"Butterflies, big data and the multifarious stressors of the Anthropocene\",\"advertiser\":\"Databricks\",\"city\":\"San Francisco\"}</td></tr><tr><td>1833247571</td><td>{\"member_id\":185424938,\"event_name\":\"KubeFlow +Keras/TensorFlow 2.0 +TF Extended (TFX) +Kubernetes +Airflow +PyTorch\",\"advertiser\":\"Databricks\",\"city\":\"San Francisco\"}</td></tr></tbody></table></div>"]}}],"execution_count":4}],"metadata":{"name":"DataPipeline_FinalProject","notebookId":4329508541514607},"nbformat":4,"nbformat_minor":0}
